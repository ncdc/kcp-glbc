[[monitoring]]
= KCP GLBC Monitoring

The KCP GLBC monitoring architecture relies on https://prometheus.io[Prometheus], https://prometheus.io/docs/alerting/latest/alertmanager/[Alertmanager] and https://grafana.com/[Grafana].

The https://prometheus-operator.dev[Prometheus operator] serves to make running Prometheus on top of Kubernetes as easy as possible, while preserving Kubernetes-native configuration options. However, because Openshift clusters include an instance of the prometheus operator already for cluster monitoring, a different approach is used for Kubernetes vs. Openshift.

[[prerequisites]]
== Prerequisites

To take full advantage of the KCP GLBC monitoring capabilities, it is recommended to have a Prometheus operator instance, that can be configured to integrate with the KCP GLBC instance deployed on the same cluster.

[[kubernetes]]
=== Kubernetes

The easiest way to get started with the Prometheus operator is by deploying it as part of https://github.com/prometheus-operator/kube-prometheus[kube-prometheus], which provisions an entire monitoring stack (including Alertmanager and Grafana).

By default, the Prometheus instance discovers applications to be monitored in the same namespace.
You can use the `podMonitorNamespaceSelector` field from the Prometheus resource to enable cross-namespace monitoring.
You may also need to specify a ServiceAccount with the `serviceAccountName` field, that's bound to a Role with the necessary permissions.

[[openshift]]
=== OpenShift

This guide assumes a Managed Openshift cluster is being used e.g. Openshift Dedicated. If using a self managed installation of Openshift (OCP), this guide is still relevant. However, you may want to use https://docs.openshift.com/container-platform/4.3/monitoring/monitoring-your-own-services.html[user workload monitoring and alerting] with OCP as it is easier to set up and configure.
For Managed Openshift, user workload monitoring has some limitations at this time. The main ones are inability to use https://docs.openshift.com/dedicated/osd_cluster_admin/osd_monitoring/osd-managing-alerts.html[alerts] or add custom grafana dashboards to the bundled monitoring stack. 

A suggested approach here is to install the https://github.com/redhat-developer/observability-operator[observability operator], which deploys its own instance of prometheus, prometheus-operator, alertmanager and grafana. This isn't ideal as there is a risk of prometheus-operator CRDs having conflicts with the cluster monitoring stack. However, the prometheus-operator CRDs are relatively stable. It's should be a reasonable stopgap until a better approach is available (such as user defined alerts in Managed Openshift). The observability operator instance of prometheus is smart enough to configure a subset of metrics to federate from the cluster prometheus. This can be useful if you want to use metrics from kube-state or node-exporter.

There is an example of installing the observability operator included in the Makefile.

[[discovery]]
== Discovery

A PodMonitor resource must be created in the same namespace as the `kcp-glbc-controller-manager` Deployment, for the Prometheus operator to reconcile, so that the managed Prometheus instance can scrape the _metrics_ endpoint.

As an example, hereafter is the PodMonitor resource that is created when executing the `kustomize build config/prometheus | kubectl apply -f -` command:

.pod_monitor.yaml
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: kcp-glbc-controller-manager
  labels: # <1>
    app.kubernetes.io/name: kcp-glbc
    app.kubernetes.io/component: controller-manager
spec:
  selector:
    matchLabels: # <2>
      app.kubernetes.io/name: kcp-glbc
      app.kubernetes.io/component: controller-manager
  podMetricsEndpoints:
    - port: metrics
----
<1> The labels must match the `podMonitorSelector` field from the Prometheus resource
<2> This label selector matches the `kcp-glbc-controller-manager` Deployment Pod template labels

The Prometheus operator https://github.com/prometheus-operator/prometheus-operator/blob/v0.56.0/Documentation/user-guides/getting-started.md#related-resources[getting started] guide documents the discovery mechanism, as well as the relationship between the operator resources.

In case the metrics are not discovered, you may want to rely on https://github.com/prometheus-operator/prometheus-operator/blob/v0.56.0/Documentation/troubleshooting.md#troubleshooting-servicemonitor-changes[Troubleshooting ServiceMonitor changes], which also applies to PodMonitor resources troubleshooting.

[[metrics]]
== Metrics

By default, KCP GLBC serves a `/metrics` HTTP endpoint on port `8080`.
This can be changed with the `--monitoring-port` option, e.g.:

[source,console]
----
$ kcp-glbc --monitoring-port=8888
----

The metrics can then be retrieved by _GETTing_ the `/metrics` endpoint, e.g.:

[source,console]
----
$ curl http://localhost:8888/metrics | grep "glbc_tls"
# HELP glbc_tls_certificate_issuance_duration_seconds GLBC TLS certificate issuance duration
# TYPE glbc_tls_certificate_issuance_duration_seconds histogram
glbc_tls_certificate_issuance_duration_seconds_bucket{issuer="letsencryptstaging",result="succeeded",le="1"} 0
glbc_tls_certificate_issuance_duration_seconds_bucket{issuer="letsencryptstaging",result="succeeded",le="5"} 0
glbc_tls_certificate_issuance_duration_seconds_bucket{issuer="letsencryptstaging",result="succeeded",le="10"} 0
glbc_tls_certificate_issuance_duration_seconds_bucket{issuer="letsencryptstaging",result="succeeded",le="15"} 0
glbc_tls_certificate_issuance_duration_seconds_bucket{issuer="letsencryptstaging",result="succeeded",le="30"} 0
glbc_tls_certificate_issuance_duration_seconds_bucket{issuer="letsencryptstaging",result="succeeded",le="45"} 0
glbc_tls_certificate_issuance_duration_seconds_bucket{issuer="letsencryptstaging",result="succeeded",le="60"} 0
glbc_tls_certificate_issuance_duration_seconds_bucket{issuer="letsencryptstaging",result="succeeded",le="120"} 1
glbc_tls_certificate_issuance_duration_seconds_bucket{issuer="letsencryptstaging",result="succeeded",le="300"} 1
glbc_tls_certificate_issuance_duration_seconds_bucket{issuer="letsencryptstaging",result="succeeded",le="+Inf"} 1
glbc_tls_certificate_issuance_duration_seconds_sum{issuer="letsencryptstaging",result="succeeded"} 93
glbc_tls_certificate_issuance_duration_seconds_count{issuer="letsencryptstaging",result="succeeded"} 1
# HELP glbc_tls_certificate_pending_request_count GLBC TLS certificate pending request count
# TYPE glbc_tls_certificate_pending_request_count gauge
glbc_tls_certificate_pending_request_count{issuer="letsencryptstaging"} 0
# HELP glbc_tls_certificate_request_errors_total GLBC TLS certificate total number of request errors
# TYPE glbc_tls_certificate_request_errors_total counter
glbc_tls_certificate_request_errors_total{issuer="letsencryptstaging"} 0
# HELP glbc_tls_certificate_request_total GLBC TLS certificate total number of requests
# TYPE glbc_tls_certificate_request_total counter
glbc_tls_certificate_request_total{issuer="letsencryptstaging",result="failed"} 0
glbc_tls_certificate_request_total{issuer="letsencryptstaging",result="succeeded"} 1
# HELP glbc_tls_certificate_secret_count GLBC TLS certificate secret count
# TYPE glbc_tls_certificate_secret_count gauge
glbc_tls_certificate_secret_count{issuer="letsencryptstaging"} 1
----

The serving of the metrics endpoint can be disabled by setting this option to `0`, e.g.:

[source,console]
----
$ kcp-glbc --monitoring-port=0
----

The KCP GLBC monitoring endpoint exposes the metrics listed in the following sections.

=== All metrics

NOTE: These are generated from a running instance of the controller using the `gen-metrics-docs` make target

include::generated_metrics.adoc[]